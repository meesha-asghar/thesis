{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import display \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,  AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "data = pd.read_pickle(r'data\\subject101.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loso\n",
    "data1 = pd.read_pickle(r'data\\data_loso1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train cross-val\n",
    "data2 = pd.read_pickle(r'data\\subject106.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = data.groupby('activity')\n",
    "activities1 = data1.groupby('activity')\n",
    "activities2 = data2.groupby('activity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut off first and last 1000 items, because activity starts end ends\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = data.activity.unique()\n",
    "#acts =  ['standing', 'walking','running','cycling']\n",
    "data = data.loc[data['activity'].isin(acts)].copy()\n",
    "for i in range(len(acts)):\n",
    "    data.drop(data[data.activity==acts[i]].iloc[:1000].index, inplace=True)\n",
    "    data.drop(data[data.activity==acts[i]].iloc[-1000:].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.loc[data1['activity'].isin(acts)].copy()\n",
    "for y in range(len(acts)):\n",
    "    data1.drop(data1[data1.activity==acts[y]].iloc[:1000].index, inplace=True)\n",
    "    data1.drop(data1[data1.activity==acts[y]].iloc[-1000:].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.loc[data2['activity'].isin(acts)].copy()\n",
    "for z in range(len(acts)):\n",
    "    data2.drop(data2[data2.activity==acts[z]].iloc[:1000].index, inplace=True)\n",
    "    data2.drop(data2[data2.activity==acts[z]].iloc[-1000:].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.drop(data.index[(data.activityID!=3) & (data.activityID!=4) & (data.activityID!=5) & (data.activityID!=6)], inplace=True)\n",
    "\n",
    "\n",
    "data.drop(data[data.activityID==3].iloc[:1000].index, inplace=True)\n",
    "data.drop(data[data.activityID==3].iloc[-1000:].index, inplace=True)\n",
    "data.drop(data[data.activityID==4].iloc[:1000].index, inplace=True)\n",
    "data.drop(data[data.activityID==4].iloc[-1000:].index, inplace=True)\n",
    "data.drop(data[data.activityID==5].iloc[:1000].index, inplace=True)\n",
    "data.drop(data[data.activityID==5].iloc[-1000:].index, inplace=True)\n",
    "data.drop(data[data.activityID==6].iloc[:1000].index, inplace=True)\n",
    "data.drop(data[data.activityID==6].iloc[-1000:].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute Acceleration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "absolute acceleration: $|a|=\\sqrt{a_x^2 + a_y^2 + a_z^2}$\n",
    "\n",
    "to get rid of the orientation of the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absacc(row):\n",
    "    return np.sqrt(row['IMU_chest_ax1']**2 + row['IMU_chest_ay1']**2 + row['IMU_chest_az1']**2)/9.806"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['absacc'] = data.apply(absacc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['absacc'] = data1.apply(absacc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['absacc'] = data2.apply(absacc, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-Min Difference of absolute Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws=512\n",
    "dt = 1.0/100.0 # the activities were with 50Hz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['accmax'] =  data['absacc'].rolling(window=ws,center=False).max() \n",
    "data['accmin'] = data['absacc'].rolling(window=ws,center=False).min() \n",
    "\n",
    "data['accmaxmindiff'] = data.accmax - data.accmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['accmax'] =  data1['absacc'].rolling(window=ws,center=False).max() \n",
    "data1['accmin'] = data1['absacc'].rolling(window=ws,center=False).min() \n",
    "\n",
    "data1['accmaxmindiff'] = data1.accmax - data1.accmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['accmax'] =  data2['absacc'].rolling(window=ws,center=False).max() \n",
    "data2['accmin'] = data2['absacc'].rolling(window=ws,center=False).min() \n",
    "\n",
    "data2['accmaxmindiff'] = data2.accmax - data2.accmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Transform of Rotation Rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_amplitude(s, kind='peak'):\n",
    "    \n",
    "    # don't forget the windowing to get rid of the leakage effect\n",
    "    hann = np.hanning(len(s)) \n",
    "    \n",
    "    # do the FFT with Hanning Window\n",
    "    Yhann = np.fft.fft(hann*s)\n",
    "    \n",
    "    N = int(len(Yhann)/2+1)\n",
    "    Y = 2*(np.abs(Yhann[:N])/N) # right half is enough info(positive freqs only)\n",
    "    \n",
    "    # frequency axis, if needed\n",
    "    fa = 1.0/dt\n",
    "    f = np.linspace(0, fa/2.0, N, endpoint=True)\n",
    "    \n",
    "    if kind=='peak':\n",
    "        return np.max(Y) # just return the maximum peak amplitude\n",
    "    elif kind=='periodicity':\n",
    "        return np.max(Y) / np.mean(Y) # return periodicity\n",
    "    elif kind=='full':\n",
    "        return f, Y # return the full spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fftamppeak'] = data['IMU_chest_rotz'].rolling(window=1*ws,center=False).apply(fft_amplitude, raw=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['fftamppeak'] = data1['IMU_chest_rotz'].rolling(window=1*ws,center=False).apply(fft_amplitude, raw=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['fftamppeak'] = data2['IMU_chest_rotz'].rolling(window=1*ws,center=False).apply(fft_amplitude, raw=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the rolling_ functions, there is overlap between the activity features and the labels, corresponding to it. We have to delete some rows (length of window), before using a classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(acts)):\n",
    "    data.drop(data[data.activity==acts[ii]].iloc[0:int(ws)-1].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iii in range(len(acts)):\n",
    "    data1.drop(data1[data1.activity==acts[iii]].iloc[0:int(ws)-1].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iiii in range(len(acts)):\n",
    "    data2.drop(data2[data2.activity==acts[iiii]].iloc[0:int(ws)-1].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.drop(data[data.activityID==3].iloc[0:int(ws)-1].index, inplace=True)\n",
    "data.drop(data[data.activityID==4].iloc[0:int(ws)-1].index, inplace=True)\n",
    "data.drop(data[data.activityID==5].iloc[0:int(ws)-1].index, inplace=True)\n",
    "data.drop(data[data.activityID==6].iloc[0:int(ws)-1].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_pickle(r'data\\data1_feats.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(df, y_col, x_cols, ratio):\n",
    "    \"\"\" \n",
    "    This method transforms a dataframe into a train and test set, for this you need to specify:\n",
    "    1. the ratio train : test (usually 0.7)\n",
    "    2. the column with the Y_values\n",
    "    \"\"\"\n",
    "#    mask = np.random.rand(len(df)) < ratio\n",
    "#    df_train = df[mask]\n",
    "#    df_test = df[~mask]\n",
    "    \n",
    "    df_train=df.sample(frac=ratio,random_state=200)\n",
    "    df_test=df.drop(df_train.index)\n",
    "       \n",
    "    Y_train = df_train[y_col].values\n",
    "    Y_test = df_test[y_col].values\n",
    "    X_train = df_train[x_cols].values\n",
    "    X_test = df_test[x_cols].values\n",
    "    return df_train, df_test, X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each ensemble algorithm is demonstrated using 10 fold cross validation, a standard technique used to estimate the performance of any machine learning algorithm on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classifiers = {\n",
    "    #\"Logistic Regression\": LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(n_neighbors =3),\n",
    "    #\"Linear SVM\": SVC(kernel='rbf', C=1.0, gamma=0.5),\n",
    "    \"Decision Tree\": tree.DecisionTreeClassifier(),\n",
    "    #\"Random Forest10\": RandomForestClassifier(n_estimators=10),\n",
    "    #\"Neural Net\": MLPClassifier(alpha = 1),\n",
    "    #\"Naive Bayes\": GaussianNB(),\n",
    "    \n",
    "    #Bagging Algos\n",
    "    \"Bagged Decision Trees\": BaggingClassifier(base_estimator=tree.DecisionTreeClassifier(), n_estimators=num_trees, random_state=seed),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=num_trees, max_features=max_features),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features),\n",
    "    #Boosting Algos\n",
    "    #\"AdaBoost\": AdaBoostClassifier(n_estimators=num_trees, random_state=seed),\n",
    "    #Stochastic Gradient Boosting\n",
    "    \"Stochastic Gradient Boosting\": GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_classify(X_train, Y_train, X_test, Y_test, no_classifiers = 5, verbose = True):\n",
    "    \"\"\"\n",
    "    This method, takes as input the X, Y matrices of the Train and Test set.\n",
    "    And fits them on all of the Classifiers specified in the dict_classifier.\n",
    "    The trained models, and accuracies are saved in a dictionary. The reason to use a dictionary\n",
    "    is because it is very easy to save the whole dictionary with the pickle module.\n",
    "    \n",
    "    Usually, the SVM, Random Forest and Gradient Boosting Classifier take quiet some time to train. \n",
    "    So it is best to train them on a smaller dataset first and \n",
    "    decide whether you want to comment them out or not based on the test accuracy score.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_models = {}\n",
    "    for classifier_name, classifier in list(dict_classifiers.items())[:no_classifiers]:\n",
    "        t_start = time.process_time()\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        t_end = time.process_time()\n",
    "        t_diff = t_end - t_start\n",
    "        \n",
    "        train_score = classifier.score(X_train, Y_train)\n",
    "        test_score = classifier.score(X_test, Y_test)\n",
    "        Y_true = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_true, Y_test)\n",
    "        \n",
    "        dict_models[classifier_name] = {'model': classifier, 'train_score': train_score, 'test_score': test_score, 'train_time': t_diff, 'y_true': Y_true, 'accuracy_score': accuracy, 'y_pred': Y_test}\n",
    "        if verbose:\n",
    "            print(\"trained {c} in {f:.2f} s\".format(c=classifier_name, f=t_diff))\n",
    "    return dict_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dict_models(dict_models, sort_by='test_score'):\n",
    "    cls = [key for key in dict_models.keys()]\n",
    "    test_s = [dict_models[key]['test_score'] for key in cls]\n",
    "    training_s = [dict_models[key]['train_score'] for key in cls]\n",
    "    training_t = [dict_models[key]['train_time'] for key in cls]\n",
    "    accuracy_s = [dict_models[key]['accuracy_score'] for key in cls]\n",
    "    \n",
    "    df_ = pd.DataFrame(data=np.zeros(shape=(len(cls),5)), columns = ['classifier', 'train_score', 'test_score', 'train_time','accuracy_score'])\n",
    "    for ii in range(0,len(cls)):\n",
    "        df_.loc[ii, 'classifier'] = cls[ii]\n",
    "        df_.loc[ii, 'train_score'] = training_s[ii]\n",
    "        df_.loc[ii, 'test_score'] = test_s[ii]\n",
    "        df_.loc[ii, 'train_time'] = training_t[ii]\n",
    "        df_.loc[ii, 'accuracy_score'] = accuracy_s[ii]*100\n",
    "    \n",
    "    display(df_.sort_values(by=sort_by, ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217076,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = data['activity'].values\n",
    "np.shape(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217076, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurevector = ['accmaxmindiff','fftamppeak']\n",
    "\n",
    "features = data[featurevector].values\n",
    "np.shape(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'activity'\n",
    "\n",
    "train_test_ratio = 0.7\n",
    "\n",
    "#df_train, df_test, features_train, labels_train, features_test, labels_test = get_train_test(data, y_col, featurevector, train_test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = data1[y_col].values\n",
    "X_train = data1[featurevector].values\n",
    "\n",
    "Y_test = data[y_col].values\n",
    "X_test = data[featurevector].values\n",
    "\n",
    "Y_train1 = data2[y_col].values\n",
    "X_train1 = data2[featurevector].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Nearest Neighbors in 2.56 s\n",
      "trained Decision Tree in 6.67 s\n",
      "trained Bagged Decision Trees in 355.72 s\n",
      "trained RF in 353.92 s\n",
      "trained Extra Trees in 90.41 s\n",
      "trained Stochastic Gradient Boosting in 1489.86 s\n"
     ]
    }
   ],
   "source": [
    "dict_models = batch_classify(X_train, Y_train, X_test, Y_test, no_classifiers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_time</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stochastic Gradient Boosting</td>\n",
       "      <td>0.769729</td>\n",
       "      <td>0.583938</td>\n",
       "      <td>1489.859375</td>\n",
       "      <td>58.393834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.495499</td>\n",
       "      <td>6.671875</td>\n",
       "      <td>49.549927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.999984</td>\n",
       "      <td>0.495232</td>\n",
       "      <td>353.921875</td>\n",
       "      <td>49.523208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagged Decision Trees</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.492698</td>\n",
       "      <td>355.718750</td>\n",
       "      <td>49.269841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.487553</td>\n",
       "      <td>90.406250</td>\n",
       "      <td>48.755275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.955330</td>\n",
       "      <td>0.473074</td>\n",
       "      <td>2.562500</td>\n",
       "      <td>47.307395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  train_score  test_score   train_time  \\\n",
       "5  Stochastic Gradient Boosting     0.769729    0.583938  1489.859375   \n",
       "1                 Decision Tree     0.999996    0.495499     6.671875   \n",
       "3                            RF     0.999984    0.495232   353.921875   \n",
       "2         Bagged Decision Trees     0.999992    0.492698   355.718750   \n",
       "4                   Extra Trees     0.999998    0.487553    90.406250   \n",
       "0             Nearest Neighbors     0.955330    0.473074     2.562500   \n",
       "\n",
       "   accuracy_score  \n",
       "5       58.393834  \n",
       "1       49.549927  \n",
       "3       49.523208  \n",
       "2       49.269841  \n",
       "4       48.755275  \n",
       "0       47.307395  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_dict_models(dict_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Nearest Neighbors in 0.53 s\n",
      "trained Decision Tree in 0.81 s\n",
      "trained Bagged Decision Trees in 36.86 s\n",
      "trained RF in 37.22 s\n",
      "trained Extra Trees in 8.30 s\n",
      "trained Stochastic Gradient Boosting in 145.19 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_time</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stochastic Gradient Boosting</td>\n",
       "      <td>0.954111</td>\n",
       "      <td>0.429375</td>\n",
       "      <td>145.187500</td>\n",
       "      <td>42.937497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.418301</td>\n",
       "      <td>8.296875</td>\n",
       "      <td>41.830050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.414247</td>\n",
       "      <td>37.218750</td>\n",
       "      <td>41.424662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagged Decision Trees</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.413708</td>\n",
       "      <td>36.859375</td>\n",
       "      <td>41.370764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.413680</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>41.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.990745</td>\n",
       "      <td>0.407581</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>40.758076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  train_score  test_score  train_time  \\\n",
       "5  Stochastic Gradient Boosting     0.954111    0.429375  145.187500   \n",
       "4                   Extra Trees     1.000000    0.418301    8.296875   \n",
       "3                            RF     1.000000    0.414247   37.218750   \n",
       "2         Bagged Decision Trees     0.999995    0.413708   36.859375   \n",
       "1                 Decision Tree     1.000000    0.413680    0.812500   \n",
       "0             Nearest Neighbors     0.990745    0.407581    0.531250   \n",
       "\n",
       "   accuracy_score  \n",
       "5       42.937497  \n",
       "4       41.830050  \n",
       "3       41.424662  \n",
       "2       41.370764  \n",
       "1       41.368000  \n",
       "0       40.758076  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_models1 = batch_classify(X_train1, Y_train1, X_test, Y_test, no_classifiers = 6)\n",
    "display_dict_models(dict_models1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, features_train, labels_train, features_test, labels_test = get_train_test(data1, y_col, featurevector, train_test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Nearest Neighbors in 2.66 s\n",
      "trained Decision Tree in 5.64 s\n",
      "trained Bagged Decision Trees in 371.83 s\n",
      "trained RF in 375.77 s\n",
      "trained Extra Trees in 120.28 s\n",
      "trained Stochastic Gradient Boosting in 1526.81 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_time</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.995253</td>\n",
       "      <td>120.281250</td>\n",
       "      <td>99.525257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.993896</td>\n",
       "      <td>375.765625</td>\n",
       "      <td>99.389616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagged Decision Trees</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.993745</td>\n",
       "      <td>371.828125</td>\n",
       "      <td>99.374545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.992841</td>\n",
       "      <td>5.640625</td>\n",
       "      <td>99.284117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.956087</td>\n",
       "      <td>0.948858</td>\n",
       "      <td>2.656250</td>\n",
       "      <td>94.885836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stochastic Gradient Boosting</td>\n",
       "      <td>0.770700</td>\n",
       "      <td>0.789983</td>\n",
       "      <td>1526.812500</td>\n",
       "      <td>78.998267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  train_score  test_score   train_time  \\\n",
       "4                   Extra Trees     0.999999    0.995253   120.281250   \n",
       "3                            RF     0.999995    0.993896   375.765625   \n",
       "2         Bagged Decision Trees     0.999994    0.993745   371.828125   \n",
       "1                 Decision Tree     0.999999    0.992841     5.640625   \n",
       "0             Nearest Neighbors     0.956087    0.948858     2.656250   \n",
       "5  Stochastic Gradient Boosting     0.770700    0.789983  1526.812500   \n",
       "\n",
       "   accuracy_score  \n",
       "4       99.525257  \n",
       "3       99.389616  \n",
       "2       99.374545  \n",
       "1       99.284117  \n",
       "0       94.885836  \n",
       "5       78.998267  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_models2 = batch_classify(features_train, labels_train, features_test, labels_test, no_classifiers = 6)\n",
    "display_dict_models(dict_models2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Store data (serialize)\n",
    "with open(r'data\\loso_class.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_models, handle)\n",
    "    \n",
    "# Store data (serialize)\n",
    "with open(r'data\\cross_class.pickle', 'wb') as handle1:\n",
    "    pickle.dump(dict_models1, handle1)\n",
    "    \n",
    "# Store data (serialize)\n",
    "with open(r'data\\sub106_class.pickle', 'wb') as handle2:\n",
    "    pickle.dump(dict_models2, handle2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
